{
  "metadata": {
    "kernelspec": {
      "language": "python",
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.12.12",
      "mimetype": "text/x-python",
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "pygments_lexer": "ipython3",
      "nbconvert_exporter": "python",
      "file_extension": ".py"
    },
    "kaggle": {
      "accelerator": "nvidiaTeslaT4",
      "dataSources": [],
      "dockerImageVersionId": 31234,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook",
      "isGpuEnabled": true
    },
    "colab": {
      "name": "YS - PEFT ",
      "provenance": []
    }
  },
  "nbformat_minor": 0,
  "nbformat": 4,
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üìù Project: Parameter-Efficient Fine-Tuning (PEFT) for Dialogue Summarization\n",
        "## üí° The Idea\n",
        "Large Language Models (LLMs) like T5 are powerful but computationally expensive to fine-tune fully. This project demonstrates how to **efficiently fine-tune Google's FLAN-T5 model** to summarize dialogues using **LoRA (Low-Rank Adaptation)**.\n",
        "\n",
        "## üéØ The Proposal (Objective)\n",
        "Instead of retraining all 248M parameters of the model, we propose to inject low-rank trainable matrices into the model.\n",
        "* **Goal:** Achieve high-quality dialogue summarization.\n",
        "* **Dataset:** `knkarthick/dialogsum`.\n",
        "* **Technique:** PEFT with LoRA configuration.\n",
        "* **Efficiency:** Train less than 1% of parameters to save GPU memory and time."
      ],
      "metadata": {
        "id": "cQf3fuHduc-y"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üó∫Ô∏è Project Outline\n",
        "1.  **Setup Environment:** Install `transformers`, `peft`, `datasets`.\n",
        "2.  **Data Preparation:** Load and tokenize the `dialogsum` dataset.\n",
        "3.  **Model Initialization:** Load pre-trained `flan-t5-base`.\n",
        "4.  **LoRA Configuration:** Apply Low-Rank Adaptation to freeze main weights and add trainable adapters.\n",
        "5.  **Training:** Fine-tune the model for 3 epochs.\n",
        "6.  **Evaluation:** Compare the PEFT model's generated summaries against human summaries."
      ],
      "metadata": {
        "id": "zIFqA2BJuc-2"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Dataset: DialogSum\n",
        "We are using the **DialogSum** dataset, which consists of real-life scenarios like doctor-patient conversations, taxi bookings, etc.\n",
        "* **Input:** A dialogue text.\n",
        "* **Target:** A human-written summary."
      ],
      "metadata": {
        "id": "9jeOG6asuc-3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from datasets import load_dataset\n",
        "from transformers import (\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    AutoTokenizer,\n",
        "    TrainingArguments,\n",
        "    Trainer,\n",
        "    GenerationConfig,\n",
        "    DataCollatorForSeq2Seq\n",
        ")\n",
        "from peft import LoraConfig, get_peft_model, TaskType, PeftModel\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:04:28.589016Z",
          "iopub.execute_input": "2025-12-26T08:04:28.589614Z",
          "iopub.status.idle": "2025-12-26T08:04:28.593305Z",
          "shell.execute_reply.started": "2025-12-26T08:04:28.589581Z",
          "shell.execute_reply": "2025-12-26T08:04:28.592746Z"
        },
        "id": "sAkXWWqBuc-3"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(\"Device:\", device)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:04:30.829849Z",
          "iopub.execute_input": "2025-12-26T08:04:30.830148Z",
          "iopub.status.idle": "2025-12-26T08:04:30.834554Z",
          "shell.execute_reply.started": "2025-12-26T08:04:30.830122Z",
          "shell.execute_reply": "2025-12-26T08:04:30.833843Z"
        },
        "id": "E3QOuzjyuc-4"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Dataset: DialogSum\n",
        "We are using the **DialogSum** dataset, which consists of real-life scenarios like doctor-patient conversations, taxi bookings, etc.\n",
        "* **Input:** A dialogue text.\n",
        "* **Target:** A human-written summary."
      ],
      "metadata": {
        "id": "UHcgsnS9uc-5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = load_dataset(\"knkarthick/dialogsum\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:04:42.278142Z",
          "iopub.execute_input": "2025-12-26T08:04:42.27868Z",
          "iopub.status.idle": "2025-12-26T08:04:45.212933Z",
          "shell.execute_reply.started": "2025-12-26T08:04:42.278651Z",
          "shell.execute_reply": "2025-12-26T08:04:45.212413Z"
        },
        "id": "5wtijBLQuc-5"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "model_name = \"google/flan-t5-base\"\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "tokenizer = AutoTokenizer.from_pretrained(model_name)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:51:17.791468Z",
          "iopub.execute_input": "2025-12-26T07:51:17.79202Z",
          "iopub.status.idle": "2025-12-26T07:51:22.82581Z",
          "shell.execute_reply.started": "2025-12-26T07:51:17.791992Z",
          "shell.execute_reply": "2025-12-26T07:51:22.82499Z"
        },
        "id": "0gF4u323uc-6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "def tokenizer_fun(ex):\n",
        "    prompts = [\n",
        "        \"Summarize the following conversation.\\n\\n\" + d + \"\\n\\nSummary:\"\n",
        "        for d in ex[\"dialogue\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs = tokenizer(\n",
        "        prompts,\n",
        "        truncation=True,\n",
        "        padding=\"max_length\",\n",
        "        max_length=512\n",
        "    )\n",
        "\n",
        "    with tokenizer.as_target_tokenizer():\n",
        "        labels = tokenizer(\n",
        "            ex[\"summary\"],\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=128\n",
        "        )\n",
        "\n",
        "    labels_ids = [\n",
        "        [(t if t != tokenizer.pad_token_id else -100) for t in seq]\n",
        "        for seq in labels[\"input_ids\"]\n",
        "    ]\n",
        "\n",
        "    model_inputs[\"labels\"] = labels_ids\n",
        "    return model_inputs\n",
        "\n",
        "# =====================\n",
        "# Tokenize dataset\n",
        "# =====================\n",
        "tokenized_datasets = data.map(\n",
        "    tokenizer_fun,\n",
        "    batched=True,\n",
        "    remove_columns=[\"id\", \"topic\", \"dialogue\", \"summary\"]\n",
        ")\n",
        "\n",
        "print(\"Sample labels:\", tokenized_datasets[\"train\"][0][\"labels\"][:20])"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:51:33.081016Z",
          "iopub.execute_input": "2025-12-26T07:51:33.08179Z",
          "iopub.status.idle": "2025-12-26T07:51:48.127682Z",
          "shell.execute_reply.started": "2025-12-26T07:51:33.081742Z",
          "shell.execute_reply": "2025-12-26T07:51:48.126827Z"
        },
        "id": "4TkgOMHmuc-6"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## ‚öôÔ∏è Methodology: Applying LoRA\n",
        "Here we apply **LoRA (Low-Rank Adaptation)**.\n",
        "* **Rank (r):** 8\n",
        "* **Alpha:** 32\n",
        "* **Trainable Parameters:** Only **0.35%** (approx 880k params) of the model will be trained, keeping the original 248M parameters frozen. This drastically reduces memory usage."
      ],
      "metadata": {
        "id": "n9O2j8uzuc-6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "lora_config = LoraConfig(\n",
        "    task_type=TaskType.SEQ_2_SEQ_LM,\n",
        "    r=8,\n",
        "    lora_alpha=32,\n",
        "    lora_dropout=0.1,\n",
        ")\n",
        "\n",
        "peft_model = get_peft_model(model, lora_config)\n",
        "peft_model.print_trainable_parameters()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:52:14.990424Z",
          "iopub.execute_input": "2025-12-26T07:52:14.991083Z",
          "iopub.status.idle": "2025-12-26T07:52:15.093461Z",
          "shell.execute_reply.started": "2025-12-26T07:52:14.991049Z",
          "shell.execute_reply": "2025-12-26T07:52:15.092551Z"
        },
        "id": "mXk2QQpIuc-7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "data_collator = DataCollatorForSeq2Seq(\n",
        "    tokenizer=tokenizer,\n",
        "    model=peft_model\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:52:15.711615Z",
          "iopub.execute_input": "2025-12-26T07:52:15.711948Z",
          "iopub.status.idle": "2025-12-26T07:52:15.715828Z",
          "shell.execute_reply.started": "2025-12-26T07:52:15.711906Z",
          "shell.execute_reply": "2025-12-26T07:52:15.714984Z"
        },
        "id": "MuWFznxFuc-7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "training_args = TrainingArguments(\n",
        "    output_dir=\"./peft-dialogue-summary-training\",\n",
        "    auto_find_batch_size=True,\n",
        "    learning_rate=1e-3,\n",
        "    num_train_epochs=3,\n",
        "    fp16=True,\n",
        "    logging_steps=10,\n",
        "    save_strategy=\"epoch\",\n",
        "    eval_strategy=\"epoch\",\n",
        "    report_to=\"none\"\n",
        ")"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:52:23.149593Z",
          "iopub.execute_input": "2025-12-26T07:52:23.15037Z",
          "iopub.status.idle": "2025-12-26T07:52:23.188478Z",
          "shell.execute_reply.started": "2025-12-26T07:52:23.150336Z",
          "shell.execute_reply": "2025-12-26T07:52:23.187804Z"
        },
        "id": "bmg3Wj_suc-7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "trainer = Trainer(\n",
        "    model=peft_model,\n",
        "    args=training_args,\n",
        "    train_dataset=tokenized_datasets[\"train\"],\n",
        "    eval_dataset=tokenized_datasets[\"test\"],\n",
        "    data_collator=data_collator,\n",
        ")\n",
        "\n",
        "print(\" Start training\")\n",
        "trainer.train()\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T07:56:02.356095Z",
          "iopub.execute_input": "2025-12-26T07:56:02.356428Z",
          "iopub.status.idle": "2025-12-26T07:56:02.360058Z",
          "shell.execute_reply.started": "2025-12-26T07:56:02.356398Z",
          "shell.execute_reply": "2025-12-26T07:56:02.359433Z"
        },
        "id": "2FW8GpK5uc-7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "peft_path = \"./peft-dialogue-summary-checkpoint\"\n",
        "trainer.model.save_pretrained(peft_path)\n",
        "tokenizer.save_pretrained(peft_path)"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:08:39.573242Z",
          "iopub.execute_input": "2025-12-26T08:08:39.573993Z",
          "iopub.status.idle": "2025-12-26T08:08:39.577036Z",
          "shell.execute_reply.started": "2025-12-26T08:08:39.573963Z",
          "shell.execute_reply": "2025-12-26T08:08:39.576403Z"
        },
        "id": "Xdvjh8kmuc-7"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "base_model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)\n",
        "peft_model = PeftModel.from_pretrained(base_model, peft_path).to(device)\n",
        "\n",
        "idx = 20\n",
        "prompt = f\"Summarize the following conversation.\\n\\n{data['test'][idx]['dialogue']}\\n\\nSummary:\"\n",
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "out = peft_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    max_new_tokens=200\n",
        ")\n",
        "print(\"Human summary:\")\n",
        "print(data[\"test\"][idx][\"summary\"])\n",
        "print(\"\\nPEFT output:\")\n",
        "print(tokenizer.decode(out[0], skip_special_tokens=True))"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:08:44.217342Z",
          "iopub.execute_input": "2025-12-26T08:08:44.217859Z",
          "iopub.status.idle": "2025-12-26T08:08:44.221124Z",
          "shell.execute_reply.started": "2025-12-26T08:08:44.217832Z",
          "shell.execute_reply": "2025-12-26T08:08:44.22043Z"
        },
        "id": "f_ydGBZ_uc-8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "code",
      "source": [
        "input_ids = tokenizer(prompt, return_tensors=\"pt\").input_ids.to(device)\n",
        "\n",
        "peft_model_outputs = peft_model.generate(\n",
        "    input_ids=input_ids,\n",
        "    generation_config=GenerationConfig(max_new_tokens=200, num_beams=1)\n",
        ")\n",
        "\n",
        "peft_model_text_output = tokenizer.decode(peft_model_outputs[0], skip_special_tokens=True)\n",
        "\n",
        "print(f\"Human Summary: {data['test'][idx]['summary']}\")\n",
        "print(f\"PEFT Model Summary: {peft_model_text_output}\")\n"
      ],
      "metadata": {
        "trusted": true,
        "execution": {
          "iopub.status.busy": "2025-12-26T08:08:47.597451Z",
          "iopub.execute_input": "2025-12-26T08:08:47.598319Z",
          "iopub.status.idle": "2025-12-26T08:08:47.601935Z",
          "shell.execute_reply.started": "2025-12-26T08:08:47.598278Z",
          "shell.execute_reply": "2025-12-26T08:08:47.601218Z"
        },
        "id": "whEfAvIGuc-8"
      },
      "outputs": [],
      "execution_count": null
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìà Results & Analysis\n",
        "\n",
        "### Training Performance\n",
        "The model was trained for 3 epochs. The loss metrics show convergence:\n",
        "* **Training Loss:** Decreased from ~1.14 to **1.07**.\n",
        "* **Validation Loss:** Stabilized around **1.24**.\n",
        "\n",
        "### Qualitative Result\n",
        "Comparing the output on a test sample (Medical Context):\n",
        "* **Input:** A conversation about symptoms (itchy, lightheaded).\n",
        "* **Human Summary:** Mentions chicken pox and hazards.\n",
        "* **PEFT Model Summary:** Successfully captures the key points: *\"#Person1# thinks #Person2# has chicken pox and #Person2# is a biohazard.\"*\n",
        "\n",
        "### Conclusion\n",
        "The implementation confirms that **PEFT/LoRA is highly effective**. We achieved coherent summarization capabilities by training only a tiny fraction of the model, making LLM customization accessible on consumer hardware (T4 GPU)."
      ],
      "metadata": {
        "id": "7DbALzI8uc-8"
      }
    }
  ]
}